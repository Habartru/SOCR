#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π GUI –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å Surya OCR
–¢–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

–ü–û–î–î–ï–†–ñ–ö–ê –ú–û–î–ï–õ–ï–ô:
- –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±–æ–π –º–æ–¥–µ–ª—å—é, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –≤ LM Studio
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –º–æ–¥–µ–ª–∏
- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π API —á–µ—Ä–µ–∑ local-model –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä

–î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —É—Å–µ—á–µ–Ω–∏—è OCR –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM: —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ñ–æ–∫—É—Å –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö/—Ä–µ–∫–≤–∏–∑–∏—Ç–∞—Ö) –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 —Å—Ç—Ä–æ–∫ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ñ–æ–∫—É—Å –Ω–∞ –ø–æ–¥–ø–∏—Å—è—Ö/–∏—Ç–æ–≥–∞—Ö).
–≠—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –¥–ª—è LLM (Phi-4 —Å 16k –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å ~99% –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–µ–ª–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏:
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF: –ò—Å–ø–æ–ª—å–∑—É–µ–º multiprocessing.Pool –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (2, –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ).
- Surya OCR: Batch processing –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –Ω–æ –≤ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥—ã–π PDF –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ.
- LLM: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ –º–æ–¥–µ–ª–µ–π –≤ LM Studio –Ω–∞ –æ–¥–Ω–æ–º –ø–æ—Ä—Ç—É (local-1 –∏ local-2). –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ round-robin –ø–æ –∏–º–µ–Ω–∞–º –º–æ–¥–µ–ª–µ–π.
"""
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import os
import json
import csv
import time
import threading
from datetime import datetime
from pathlib import Path
import requests
import multiprocessing  # –î–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
from multiprocessing import Process, Queue
import queue

# –ò–º–ø–æ—Ä—Ç—ã Surya
from surya.input.load import load_from_file
from surya.detection import DetectionPredictor
from surya.recognition import RecognitionPredictor
from surya.common.surya.schema import TaskNames

# –ò–º–ø–æ—Ä—Ç –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
from token_counter import smart_truncate_for_llm, check_context_limit


def ocr_worker(pdf_queue, ocr_queue, pdf_folder, date_format):
    """OCR –≤–æ—Ä–∫–µ—Ä: –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –∏ –ø–æ–¥–∞–µ—Ç –≤ OCR –æ—á–µ—Ä–µ–¥—å"""
    det_predictor = DetectionPredictor()
    rec_predictor = RecognitionPredictor()
    
    while True:
        try:
            pdf_file = pdf_queue.get(timeout=1)
            if pdf_file is None:  # –°–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                break
                
            pdf_path = os.path.join(pdf_folder, pdf_file)
            
            # OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞
            images, names = load_from_file(pdf_path)
            task_names = [TaskNames.ocr_with_boxes] * len(images)
            predictions = rec_predictor(images, task_names=task_names, det_predictor=det_predictor, math_mode=False)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            pages_data, combined_text = [], ""
            for page_idx, pred in enumerate(predictions):
                page_lines = [{"text": line.text, "bbox": line.bbox, "confidence": line.confidence} for line in pred.text_lines]
                pages_data.append({"page": page_idx + 1, "text_lines": page_lines})
                combined_text += " ".join(line.text for line in pred.text_lines) + " "
            
            ocr_json = {"filename": pdf_file, "pages": len(predictions), "pages_data": pages_data, "full_text": combined_text.strip()}
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV
            csv_file = os.path.join(os.path.dirname(pdf_folder), "ocr_result.csv")
            with open(csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                if not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:
                    writer.writerow(['filename', 'recognition_date', 'ocr_json', 'ocr_text'])
                date_str = datetime.now().strftime("%Y-%m-%d" if date_format == "ISO" else "%d.%m.%Y")
                writer.writerow([pdf_file, date_str, json.dumps(ocr_json, ensure_ascii=False), combined_text.strip()])
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê!)
            if len(pages_data) == 1:
                # –û–î–ù–ê –°–¢–†–ê–ù–ò–¶–ê - –ø–µ—Ä–µ–¥–∞–µ–º –ü–û–õ–ù–û–°–¢–¨–Æ
                truncated_lines = [{"source": "single_page", **line} for line in pages_data[0]["text_lines"]]
            else:
                # –ú–ù–û–ì–û–°–¢–†–ê–ù–ò–ß–ù–´–ï - —Ç–æ–ª—å–∫–æ 10 –ø–µ—Ä–≤—ã—Ö + 30 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
                truncated_lines = []
                first_page_lines = pages_data[0]["text_lines"]
                truncated_lines.extend([{"source": "first_page", **line} for line in first_page_lines[:10]])
                last_page_lines = pages_data[-1]["text_lines"]
                start_idx = max(0, len(last_page_lines) - 30)
                truncated_lines.extend([{"source": "last_page", **line} for line in last_page_lines[start_idx:]])
            
            # –ü–æ–¥–∞–µ–º –≤ OCR –æ—á–µ—Ä–µ–¥—å
            ocr_queue.put((pdf_file, truncated_lines, combined_text.strip()))
            
        except queue.Empty:
            continue
        except Exception as e:
            ocr_queue.put((pdf_file, None, f"OCR –æ—à–∏–±–∫–∞: {e}"))

def llm_worker(ocr_queue, result_queue, json_folder, llm_settings, model_name, worker_name=None, retry_queue=None):
    """–õ–õ–ú –≤–æ—Ä–∫–µ—Ä: –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç OCR –¥–∞–Ω–Ω—ã–µ"""
    display_name = worker_name or model_name
    result_queue.put(f"–ó–∞–ø—É—â–µ–Ω LLM –≤–æ—Ä–∫–µ—Ä: {display_name}")
    
    while True:
        try:
            item = ocr_queue.get(timeout=1)
            if item is None:  # –°–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                result_queue.put(f"–ó–∞–≤–µ—Ä—à–∞–µ–º LLM –≤–æ—Ä–∫–µ—Ä: {display_name}")
                break
                
            # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ —Å —É—á–µ—Ç–æ–º —Å—á–µ—Ç—á–∏–∫–∞ –ø–æ–ø—ã—Ç–æ–∫
            if len(item) == 4:
                pdf_file, truncated_data, combined_text, retry_count = item
            else:
                pdf_file, truncated_data, combined_text = item
                retry_count = 0
                
            result_queue.put(f"–ü–æ–ª—É—á–∏–ª –∑–∞–¥–∞–Ω–∏–µ [{display_name}]: {pdf_file} (–ø–æ–ø—ã—Ç–∫–∞ {retry_count + 1})")
            
            if truncated_data is None:  # –û—à–∏–±–∫–∞ OCR
                result_queue.put(f"{pdf_file}: {combined_text}")
                continue
            
            # –ê–Ω–∞–ª–∏–∑ —Å LLM (—Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏)
            start_time = time.time()
            llm_result = analyze_with_llm_worker(pdf_file, truncated_data, llm_settings, model_name)
            processing_time = time.time() - start_time
            
            if "error" not in llm_result:
                llm_result = validate_llm_result(llm_result, combined_text)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON
                json_filename = os.path.splitext(pdf_file)[0] + ".json"
                json_path = os.path.join(json_folder, json_filename)
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(llm_result, f, ensure_ascii=False, indent=2)
                
                # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                doc_type = llm_result.get("–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞", llm_result.get("–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞", "–Ω–µ —É–∫–∞–∑–∞–Ω"))
                result_queue.put(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ [{display_name}]: {pdf_file} (–≤—Ä–µ–º—è: {processing_time:.1f}—Å) - {doc_type}")
            else:
                # –û—à–∏–±–∫–∞ LLM - –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–∞
                max_retries = llm_settings.get('max_retries', 3)
                auto_retry = llm_settings.get('auto_retry', True)
                
                # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: retry_count –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0, –º–∞–∫—Å–∏–º—É–º max_retries –ø–æ–ø—ã—Ç–æ–∫
                if auto_retry and retry_count < max_retries and retry_queue is not None:
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –ø–æ–≤—Ç–æ—Ä–∞
                    retry_queue.put((pdf_file, truncated_data, combined_text, retry_count + 1))
                    result_queue.put(f"–ü–æ–≤—Ç–æ—Ä [{display_name}] –¥–ª—è {pdf_file}: {llm_result['error']} (–ø–æ–ø—ã—Ç–∫–∞ {retry_count + 2}/{max_retries + 1})")
                else:
                    # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –∏—Å—á–µ—Ä–ø–∞–Ω –∏–ª–∏ –∞–≤—Ç–æ–ø–æ–≤—Ç–æ—Ä –æ—Ç–∫–ª—é—á–µ–Ω
                    if "prediction-error" in llm_result.get('error', '').lower():
                        result_queue.put(f"–û—à–∏–±–∫–∞ LLM [{display_name}] –¥–ª—è {pdf_file}: –ü—Ä–µ–≤—ã—à–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ - –¥–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–≤—Ä–µ–º—è: {processing_time:.1f}—Å, –ø–æ–ø—ã—Ç–∫–∞ {retry_count + 1}/{max_retries + 1})")
                    else:
                        result_queue.put(f"–û—à–∏–±–∫–∞ LLM [{display_name}] –¥–ª—è {pdf_file}: {llm_result['error']} (–≤—Ä–µ–º—è: {processing_time:.1f}—Å, –ø–æ–ø—ã—Ç–∫–∞ {retry_count + 1}/{max_retries + 1})")
                
        except queue.Empty:
            continue
        except Exception as e:
            result_queue.put(f"–û—à–∏–±–∫–∞ [{display_name}] {pdf_file}: {e}")

def ocr_worker_simple(pdf_queue, result_queue):
    """–ü—Ä–æ—Å—Ç–æ–π OCR –≤–æ—Ä–∫–µ—Ä –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    det_predictor = DetectionPredictor()
    rec_predictor = RecognitionPredictor()
    
    while True:
        try:
            item = pdf_queue.get(timeout=1)
            if item is None:  # –°—Ç–æ–ø-—Å–∏–≥–Ω–∞–ª
                break
                
            pdf_file, pdf_folder, date_format = item
            result = ocr_single_file_worker(pdf_file, pdf_folder, date_format)
            result_queue.put(result)
            
        except queue.Empty:
            continue
        except Exception as e:
            result_queue.put({
                "success": False,
                "filename": "unknown",
                "error": str(e)
            })

def ocr_single_file_worker(pdf_file, pdf_folder, date_format):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ PDF —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ Surya OCR (–≤–Ω–µ –∫–ª–∞—Å—Å–∞)"""
    start_time = time.time()
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Surya –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
        det_predictor = DetectionPredictor()
        rec_predictor = RecognitionPredictor()
        
        pdf_path = os.path.join(pdf_folder, pdf_file)
        
        # OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞
        images, names = load_from_file(pdf_path)
        task_names = [TaskNames.ocr_with_boxes] * len(images)
        predictions = rec_predictor(
            images,
            task_names=task_names,
            det_predictor=det_predictor,
            math_mode=False
        )
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        pages_data = []
        combined_text = ""
        
        for page_idx, pred in enumerate(predictions):
            page_lines = []
            page_text = ""
            for line in pred.text_lines:
                line_data = {
                    "text": line.text,
                    "bbox": line.bbox,
                    "confidence": line.confidence
                }
                page_lines.append(line_data)
                page_text += line.text + " "
            pages_data.append({
                "page": page_idx + 1,
                "text_lines": page_lines
            })
            combined_text += page_text
        
        ocr_json = {
            "filename": os.path.basename(pdf_path),
            "pages": len(predictions),
            "pages_data": pages_data,
            "full_text": combined_text.strip()
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
        csv_file = os.path.join(os.path.dirname(pdf_folder), "ocr_result.csv")
        file_exists = os.path.exists(csv_file)
        
        with open(csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(['filename', 'recognition_date', 'ocr_json', 'ocr_text'])
            
            date_str = datetime.now().strftime("%Y-%m-%d" if date_format == "ISO" else "%d.%m.%Y")
            writer.writerow([pdf_file, date_str, json.dumps(ocr_json, ensure_ascii=False), combined_text.strip()])
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM —Å —É–º–Ω—ã–º —É—Å–µ—á–µ–Ω–∏–µ–º
        all_lines = []
        for page_data in pages_data:
            all_lines.extend(page_data["text_lines"])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω–æ–µ —É—Å–µ—á–µ–Ω–∏–µ —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º —Ç–æ–∫–µ–Ω–æ–≤
        max_tokens = 12000  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (4000 —Ç–æ–∫–µ–Ω–æ–≤)
        truncated_lines, token_count, was_truncated = smart_truncate_for_llm(all_lines, max_tokens)
        
        if was_truncated:
            print(f"‚úÇÔ∏è –î–æ–∫—É–º–µ–Ω—Ç {pdf_file} —É—Å–µ—á–µ–Ω: {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
        else:
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {pdf_file} –ø–æ–º–µ—â–∞–µ—Ç—Å—è: {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
        
        processing_time = time.time() - start_time
        return {
            "success": True,
            "filename": pdf_file,
            "truncated_data": truncated_lines,
            "combined_text": combined_text.strip(),
            "processing_time": processing_time
        }
        
    except Exception as e:
        processing_time = time.time() - start_time
        return {
            "success": False,
            "filename": pdf_file,
            "error": str(e),
            "processing_time": processing_time
        }

def process_single_file_worker(args):
    """–§—É–Ω–∫—Ü–∏—è-–≤–æ—Ä–∫–µ—Ä –¥–ª—è multiprocessing (–≤–Ω–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è pickle –æ—à–∏–±–æ–∫)"""
    pdf_file, pdf_folder, json_folder, date_format, llm_settings = args
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Surya –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
        det_predictor = DetectionPredictor()
        rec_predictor = RecognitionPredictor()
        
        pdf_path = os.path.join(pdf_folder, pdf_file)
        
        # OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞
        images, names = load_from_file(pdf_path)
        task_names = [TaskNames.ocr_with_boxes] * len(images)
        predictions = rec_predictor(
            images,
            task_names=task_names,
            det_predictor=det_predictor,
            math_mode=False
        )
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        pages_data = []
        combined_text = ""
        
        for page_idx, pred in enumerate(predictions):
            page_lines = []
            page_text = ""
            for line in pred.text_lines:
                line_data = {
                    "text": line.text,
                    "bbox": line.bbox,
                    "confidence": line.confidence
                }
                page_lines.append(line_data)
                page_text += line.text + " "
            pages_data.append({
                "page": page_idx + 1,
                "text_lines": page_lines
            })
            combined_text += page_text
        
        ocr_json = {
            "filename": os.path.basename(pdf_path),
            "pages": len(predictions),
            "pages_data": pages_data,
            "full_text": combined_text.strip()
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
        csv_file = os.path.join(os.path.dirname(pdf_folder), "ocr_result.csv")
        file_exists = os.path.exists(csv_file)
        
        with open(csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(['filename', 'recognition_date', 'ocr_json', 'ocr_text'])
            
            date_str = datetime.now().strftime("%Y-%m-%d" if date_format == "ISO" else "%d.%m.%Y")
            writer.writerow([pdf_file, date_str, json.dumps(ocr_json, ensure_ascii=False), combined_text.strip()])
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM —Å —É–º–Ω—ã–º —É—Å–µ—á–µ–Ω–∏–µ–º
        all_lines = []
        for page_data in pages_data:
            all_lines.extend(page_data["text_lines"])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω–æ–µ —É—Å–µ—á–µ–Ω–∏–µ
        max_tokens = 12000  # –û—Å—Ç–∞–≤–ª—è–µ–º 4000 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        truncated_lines, token_count, was_truncated = smart_truncate_for_llm(all_lines, max_tokens)
        
        if was_truncated:
            print(f"‚úÇÔ∏è –î–æ–∫—É–º–µ–Ω—Ç {pdf_file} —É—Å–µ—á–µ–Ω: {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
        else:
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {pdf_file} –ø–æ–º–µ—â–∞–µ—Ç—Å—è: {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –ê–Ω–∞–ª–∏–∑ —Å LLM
        model_name = llm_settings.get('models', ['local-1'])[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å
        llm_result = analyze_with_llm_worker(pdf_file, truncated_lines, llm_settings, model_name)
        
        if "error" not in llm_result:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            llm_result = validate_llm_result(llm_result, combined_text.strip())
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON
            json_filename = os.path.splitext(pdf_file)[0] + ".json"
            json_path = os.path.join(json_folder, json_filename)
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(llm_result, f, ensure_ascii=False, indent=2)
            
            return f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {pdf_file}"
        else:
            return f"–û—à–∏–±–∫–∞ LLM –¥–ª—è {pdf_file}: {llm_result['error']}"
            
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file}: {str(e)}"


def generate_llm_prompt(filename, truncated_data, structured_data):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM —Å —á–µ—Ç–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–æ–ª–µ–π –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–µ–ª–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º.

–í–ê–ñ–ù–û: –í JSON –æ—Ç–≤–µ—Ç–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¢–û–õ–¨–ö–û –û–î–ù–û –ø–æ–ª–µ —Å —Ç–∏–ø–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ - "–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞" - –±–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏–π, –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤!

–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –†–û–õ–ï–ô (–°–ê–ú–û–ï –í–ê–ñ–ù–û–ï) –í –∫–∞–∂–¥–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤—Å–µ–≥–¥–∞ 2 —Å—Ç–æ—Ä–æ–Ω—ã –∏ –≤ –æ—Ç–≤–µ—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 2 —Å—Ç–æ—Ä–æ–Ω—ã:
1. –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨ (–∫—Ç–æ –æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å–ª—É–≥–∏/–ø—Ä–æ–¥–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã):
   - –í –¥–æ–≥–æ–≤–æ—Ä–µ: "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å", "–ü—Ä–æ–¥–∞–≤–µ—Ü", "–ü–æ—Å—Ç–∞–≤—â–∏–∫", –∏–∑—É—á–∞–π —Å—Ç–æ—Ä–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–∞ —á—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å.
   - –í –∞–∫—Ç–µ: "–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å", —Ç–æ—Ç –∫—Ç–æ –≤—ã–ø–æ–ª–Ω–∏–ª —Ä–∞–±–æ—Ç—ã/–æ–∫–∞–∑–∞–ª —É—Å–ª—É–≥–∏
   - –í —Å—á–µ—Ç–µ: "–ü–æ—Å—Ç–∞–≤—â–∏–∫", "–ò–ü" –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≤ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ —Å—á–µ—Ç–∞

2. –ó–ê–ö–ê–ó–ß–ò–ö (–∫—Ç–æ –ø–ª–∞—Ç–∏—Ç/–ø–æ–ª—É—á–∞–µ—Ç —É—Å–ª—É–≥–∏/—Ç–æ–≤–∞—Ä—ã):
   - –í –¥–æ–≥–æ–≤–æ—Ä–µ: "–ó–∞–∫–∞–∑—á–∏–∫", "–ü–æ–∫—É–ø–∞—Ç–µ–ª—å", "–ü–æ–ª—É—á–∞—Ç–µ–ª—å", —Å—Ç–æ—Ä–æ–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∏–∑—É—á–∏ —á—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å –∫—Ç–æ –µ—Å—Ç—å –∫—Ç–æ.
   - –í –∞–∫—Ç–µ: "–ó–∞–∫–∞–∑—á–∏–∫", —Ç–æ—Ç –∫–æ–º—É –æ–∫–∞–∑–∞–Ω—ã —É—Å–ª—É–≥–∏
   - –í —Å—á–µ—Ç–µ: "–ü–æ–∫—É–ø–∞—Ç–µ–ª—å", "–ü–ª–∞—Ç–µ–ª—å—â–∏–∫", –ø–æ–ª—É—á–∞—Ç–µ–ª—å —Å—á–µ—Ç–∞

–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê –î–û–ö–£–ú–ï–ù–¢–ê (–¢–û–õ–¨–ö–û –û–î–ò–ù –¢–ò–ü - –ë–ï–ó –î–£–ë–õ–ò–†–û–í–ê–ù–ò–Ø!):
- "–¥–æ–≥–æ–≤–æ—Ä" - –µ—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –µ—Å—Ç—å —Å–ª–æ–≤–∞ "–¥–æ–≥–æ–≤–æ—Ä", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "–∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏"
- "–∞–∫—Ç" - –µ—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –µ—Å—Ç—å —Å–ª–æ–≤–∞ "–∞–∫—Ç", "–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç", "–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö —É—Å–ª—É–≥"
- "—Å—á–µ—Ç" - –µ—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –µ—Å—Ç—å "—Å—á–µ—Ç", "—Å—á–µ—Ç –Ω–∞ –æ–ø–ª–∞—Ç—É"
- "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞" - –µ—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –µ—Å—Ç—å "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞"

–¢–ò–ü–´ –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô (–æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ—á–Ω–æ):
- "—é—Ä–ª–∏—Ü–æ" - –û–û–û, –ê–û, –ü–ê–û, –ì–£–ü, –ú–£–ü, –§–ì–£–ü (–ò–ù–ù 10 —Ü–∏—Ñ—Ä, –ö–ü–ü 9 —Ü–∏—Ñ—Ä)
- "–∏–ø" - –ò–ü, –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å (–ò–ù–ù 12 —Ü–∏—Ñ—Ä, –±–µ–∑ –ö–ü–ü)
- "—Ñ–∏–∑–ª–∏—Ü–æ" - –§.–ò.–û. –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –û–û–û/–ò–ü (–≤–æ–∑–º–æ–∂–Ω–æ –ò–ù–ù 12 —Ü–∏—Ñ—Ä)

–ü–†–ê–í–ò–õ–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –î–ê–ù–ù–´–•:
- –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: —Ç–æ–ª—å–∫–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–û–û–û "–ù–∞–∑–≤–∞–Ω–∏–µ", –ò–ü –ò–≤–∞–Ω–æ–≤ –ê.–ê., –ª–∏–±–æ —Ñ–∏–∑ –ª–∏—Ü–æ - –ò–≤–∞–Ω–æ–≤ –ê.–ê.)
- –ò–ù–ù: —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞ –¥–ª–∏–Ω–æ–π 10 —Ü–∏—Ñ—Ä (—é—Ä–ª–∏—Ü–∞) –∏–ª–∏ 12 —Ü–∏—Ñ—Ä (–ò–ü/—Ñ–∏–∑–ª–∏—Ü–∞)
- –ö–ü–ü: —Ç–æ–ª—å–∫–æ 9 —Ü–∏—Ñ—Ä (—Ç–æ–ª—å–∫–æ –¥–ª—è —é—Ä–ª–∏—Ü!)
- –ê–¥—Ä–µ—Å: –ø–æ–ª–Ω—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–¥—Ä–µ—Å –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏

–ü–†–ò–ú–ï–†–´ –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –†–û–õ–ï–ô:
- –í –¥–æ–≥–æ–≤–æ—Ä–µ –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è: –ü—Ä–æ–¥–∞–≤–µ—Ü = –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨, –ü–æ–∫—É–ø–∞—Ç–µ–ª—å = –ó–ê–ö–ê–ó–ß–ò–ö
- –í –∞–∫—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç: –∫—Ç–æ –≤—ã–ø–æ–ª–Ω–∏–ª —Ä–∞–±–æ—Ç—ã = –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨, –∫—Ç–æ –ø—Ä–∏–Ω—è–ª = –ó–ê–ö–ê–ó–ß–ò–ö
- –í —Å—á–µ—Ç–µ: –∫—Ç–æ –≤—ã—Å—Ç–∞–≤–∏–ª —Å—á–µ—Ç = –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨, –∫–æ–º—É –≤—ã—Å—Ç–∞–≤–ª–µ–Ω —Å—á–µ—Ç = –ó–ê–ö–ê–ó–ß–ò–ö

–î–û–ö–£–ú–ï–ù–¢:
{structured_data}

–í–ï–†–ù–ò –°–¢–†–û–ì–û –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ù–´–ô JSON –í –¢–û–ß–ù–û–ú –°–û–û–¢–í–ï–¢–°–¢–í–ò–ò –° –®–ê–ë–õ–û–ù–û–ú –ù–ò–ñ–ï. –¢–û–ß–ù–û –°–û–ë–õ–Æ–î–ê–ô –ò–ú–ï–ù–ê –ü–û–õ–ï–ô (—Å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ–º, –Ω–µ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏):
{{
  "–ù–∞–∑–≤–∞–Ω–∏–µ_—Ñ–∞–π–ª–∞": "{filename}",
  "–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞": "–¥–æ–≥–æ–≤–æ—Ä",  // –¢–û–õ–¨–ö–û –û–î–ù–û –ü–û–õ–ï –° –¢–ò–ü–û–ú! –ò—Å–ø–æ–ª—å–∑—É–π "–¥–æ–≥–æ–≤–æ—Ä", "–∞–∫—Ç", "—Å—á–µ—Ç" –∏–ª–∏ "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞" –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
  "–ù–æ–º–µ—Ä_–¥–æ–∫—É–º–µ–Ω—Ç–∞": "",
  "–î–∞—Ç–∞_–¥–æ–∫—É–º–µ–Ω—Ç–∞": "",
  "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ_–∑–∞–∫–∞–∑—á–∏–∫–∞": "",
  "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "",
  "–ò–ù–ù_–∑–∞–∫–∞–∑—á–∏–∫–∞": "",  // 10 –∏–ª–∏ 12 —Ü–∏—Ñ—Ä, –Ω–µ –ø—É—Ç–∞–π —Å –ö–ü–ü!
  "–ö–ü–ü_–∑–∞–∫–∞–∑—á–∏–∫–∞": "",  // 9 —Ü–∏—Ñ—Ä, —Ç–æ–ª—å–∫–æ –¥–ª—è —é—Ä–ª–∏—Ü
  "–ê–¥—Ä–µ—Å_–∑–∞–∫–∞–∑—á–∏–∫–∞": "",
  "–ò–ù–ù_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "",  // 10 –∏–ª–∏ 12 —Ü–∏—Ñ—Ä, –Ω–µ –ø—É—Ç–∞–π —Å –ö–ü–ü!
  "–ö–ü–ü_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "",  // 9 —Ü–∏—Ñ—Ä, —Ç–æ–ª—å–∫–æ –¥–ª—è —é—Ä–ª–∏—Ü
  "–ê–¥—Ä–µ—Å_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "",
  "–¢–∏–ø_–∑–∞–∫–∞–∑—á–∏–∫–∞": "—é—Ä–ª–∏—Ü–æ",  // —Å—Ç—Ä–æ–≥–æ –æ–¥–Ω–æ –∏–∑: —é—Ä–ª–∏—Ü–æ, –∏–ø, —Ñ–∏–∑–ª–∏—Ü–æ
  "–¢–∏–ø_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "—é—Ä–ª–∏—Ü–æ"  // —Å—Ç—Ä–æ–≥–æ –æ–¥–Ω–æ –∏–∑: —é—Ä–ª–∏—Ü–æ, –∏–ø, —Ñ–∏–∑–ª–∏—Ü–æ
}}"""
    return prompt


def analyze_document(filename, truncated_data, llm_settings, model_name):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å LLM"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
        if not truncated_data or len(truncated_data) == 0:
            return {"error": "–ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ OCR"}
        
        structured_data = json.dumps(truncated_data, ensure_ascii=False, indent=2)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        data_size = len(structured_data)
        print(f"üìä –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM: {filename}, —Ä–∞–∑–º–µ—Ä {data_size} –±–∞–π—Ç")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = generate_llm_prompt(filename, truncated_data, structured_data)
        
        return send_to_llm(prompt, llm_settings, model_name)
        
    except Exception as e:
        return {"error": str(e)}


def analyze_with_llm_worker(filename, truncated_data, llm_settings, model_name):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å LLM"""
    return analyze_document(filename, truncated_data, llm_settings, model_name)


def send_to_llm(prompt, llm_settings, model_name):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –≤ LLM –∏ –Ω–∞–¥–µ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ JSON-–æ—Ç–≤–µ—Ç–æ–≤"""
    try:
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ OpenAI –∏ LM Studio
        provider = llm_settings.get('provider', 'LM Studio')
        
        if provider == 'OpenAI':
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è OpenAI
            api_key = llm_settings.get('api_key', '')
            if not api_key:
                return {"error": "–ù–µ —É–∫–∞–∑–∞–Ω OpenAI API –∫–ª—é—á"}
                
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            endpoint = "https://api.openai.com/v1/chat/completions"
        else:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è LM Studio
            headers = {"Content-Type": "application/json"}
            endpoint = f"{llm_settings.get('endpoint', 'http://localhost:1234')}/v1/chat/completions"
        
        data = {
            "model": model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
            "max_tokens": llm_settings.get('max_tokens', 16000)
        }
        
        try:
            response = requests.post(
                endpoint, headers=headers, json=data, 
                timeout=llm_settings.get('timeout', 180)
            )
        except requests.exceptions.Timeout:
            return {"error": f"–¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {provider} (–±–æ–ª–µ–µ 180—Å)"}
        except requests.exceptions.ConnectionError:
            return {"error": f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ {provider} - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ LM Studio"}
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ —Å {provider}: {e}"}
        
        if response.status_code == 200:
            try:
                result = response.json()
                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                
                if not content:
                    return {"error": "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏"}
                
                # –ü–æ–∏—Å–∫ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON-–±–ª–æ–∫–∞
                start = content.find('{')
                end = content.rfind('}') + 1
                
                if start != -1 and end != -1:
                    json_str = content[start:end]
                    
                    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –æ—à–∏–±–æ–∫ JSON
                    json_str = fix_json_format(json_str)
                    
                    try:
                        return json.loads(json_str)
                    except json.JSONDecodeError as e:
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
                        json_str = aggressive_json_repair(json_str)
                        try:
                            return json.loads(json_str)
                        except json.JSONDecodeError:
                            # –ï—Å–ª–∏ —Å–Ω–æ–≤–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏
                            return {"error": f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {e}\n–§—Ä–∞–≥–º–µ–Ω—Ç –æ—Ç–≤–µ—Ç–∞: {json_str[:100]}..."}
                else:
                    return {"error": f"–ù–µ –Ω–∞–π–¥–µ–Ω JSON –≤ –æ—Ç–≤–µ—Ç–µ: {content[:200]}..."}
                
            except json.JSONDecodeError as e:
                return {"error": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"}
            except Exception as e:
                return {"error": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞: {e}"}
                
        elif response.status_code == 400:
            # –û—à–∏–±–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (LM Studio)
            try:
                error_detail = response.json().get('error', '')
                if 'context' in error_detail.lower() or 'token' in error_detail.lower():
                    return {"error": "prediction-error: –ü—Ä–µ–≤—ã—à–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏"}
                else:
                    return {"error": f"prediction-error: {error_detail}"}
            except:
                return {"error": "prediction-error: –ü—Ä–µ–≤—ã—à–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏"}
        elif response.status_code == 422:
            # –û—à–∏–±–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (OpenAI)
            try:
                error_detail = response.json().get('detail', '')
                if 'context' in error_detail.lower() or 'token' in error_detail.lower():
                    return {"error": "prediction-error: –ü—Ä–µ–≤—ã—à–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏"}
                else:
                    return {"error": f"prediction-error: {error_detail}"}
            except:
                return {"error": "prediction-error: –ü—Ä–µ–≤—ã—à–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏"}
        else:
            try:
                error_detail = response.json()
                return {"error": f"HTTP {response.status_code}: {error_detail}"}
            except:
                return {"error": f"HTTP {response.status_code}: {response.text[:200]}"}
        
    except Exception as e:
        return {"error": str(e)}


def fix_json_format(json_str):
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ –≤ JSON-—Å—Ç—Ä–æ–∫–µ"""
    import re
    
    # –ó–∞–º–µ–Ω—è–µ–º –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–π –∏ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    json_str = re.sub(r"([\{\s,]+)'([^']+)'\s*:", r'\1"\2":', json_str)
    json_str = re.sub(r":\s*'([^']+)'([\s,\}]+)", r':"\1"\2', json_str)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–ª—é—á–∏ –±–µ–∑ –∫–∞–≤—ã—á–µ–∫ (–Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–∞—è –æ—à–∏–±–∫–∞)
    json_str = re.sub(r"([\{\s,]+)([A-Za-z–ê-–Ø–∞-—è0-9_]+)\s*:", r'\1"\2":', json_str)
    
    # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ —Å—Ç–∏–ª–µ JavaScript
    json_str = re.sub(r"//[^\n]*", "", json_str)
    
    # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø—è—Ç—ã–µ –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–º–∏ —Å–∫–æ–±–∫–∞–º–∏ (trailing commas)
    json_str = re.sub(r',\s*\}', '}', json_str)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º \n –≤ –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    in_string = False
    result = []
    for char in json_str:
        if char == '"' and (not result or result[-1] != '\\'):
            in_string = not in_string
        if in_string and char == '\n':
            result.append(' ')
        else:
            result.append(char)
    
    return ''.join(result)


def aggressive_json_repair(json_str):
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ JSON –ø—Ä–∏ —Å–µ—Ä—å–µ–∑–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö —Ñ–æ—Ä–º–∞—Ç–∞"""
    import re
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    json_str = fix_json_format(json_str)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏
    json_str = re.sub(r'("[^"]*")\s*("[^"]*"\s*:)', r'\1,\2', json_str)
    
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
    in_string = False
    quote_start = -1
    result = []
    
    for i, char in enumerate(json_str):
        if char == '"' and (i == 0 or json_str[i-1] != '\\'):
            if not in_string:
                in_string = True
                quote_start = i
            else:
                in_string = False
                
        # –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏ –∏ –Ω–∞—à–ª–∏ –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–∞–≤—ã—á–∫—É - —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º –µ—ë
        if in_string and char == '"' and i != quote_start and json_str[i-1] != '\\':
            result.append('\\')
            
        result.append(char)
    
    return ''.join(result)


def validate_llm_result(llm_result, original_text):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π"""
    if "error" in llm_result:
        return llm_result
        
    try:
        import re
        normalized_result = {}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç —Å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ–º
        key_mapping = {
            # –ö–ª—é—á–∏ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ -> –∫–ª—é—á–∏ —Å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ–º
            "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞": "–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞",
            "–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞": "–ù–æ–º–µ—Ä_–¥–æ–∫—É–º–µ–Ω—Ç–∞",
            "–î–∞—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞": "–î–∞—Ç–∞_–¥–æ–∫—É–º–µ–Ω—Ç–∞",
            "–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞": "–ù–∞–∑–≤–∞–Ω–∏–µ_—Ñ–∞–π–ª–∞",
            "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞": "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ_–∑–∞–∫–∞–∑—á–∏–∫–∞",
            "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è",
            "–ò–ù–ù –∑–∞–∫–∞–∑—á–∏–∫–∞": "–ò–ù–ù_–∑–∞–∫–∞–∑—á–∏–∫–∞",
            "–ö–ü–ü –∑–∞–∫–∞–∑—á–∏–∫–∞": "–ö–ü–ü_–∑–∞–∫–∞–∑—á–∏–∫–∞",
            "–ê–¥—Ä–µ—Å –∑–∞–∫–∞–∑—á–∏–∫–∞": "–ê–¥—Ä–µ—Å_–∑–∞–∫–∞–∑—á–∏–∫–∞",
            "–ò–ù–ù –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "–ò–ù–ù_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è",
            "–ö–ü–ü –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "–ö–ü–ü_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è",
            "–ê–¥—Ä–µ—Å –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "–ê–¥—Ä–µ—Å_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è",
            "–¢–∏–ø –∑–∞–∫–∞–∑—á–∏–∫–∞": "–¢–∏–ø_–∑–∞–∫–∞–∑—á–∏–∫–∞",
            "–¢–∏–ø –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "–¢–∏–ø_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è"
        }
        
        # –ü–µ—Ä–µ–Ω–æ—Å –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∫–ª—é—á–µ–π
        for key, value in llm_result.items():
            normalized_key = key_mapping.get(key, key)
            normalized_result[normalized_key] = value
        
        # –ï—Å–ª–∏ –±—ã–ª –¥—É–±–ª–∏—Ä—É—é—â–∏–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∏—Å–ø—Ä–∞–≤–ª—è–µ–º
        doc_type_keys = ["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞", "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"]
        found_doc_type = None
        for key in doc_type_keys:
            if key in normalized_result:
                found_doc_type = normalized_result[key].lower()
                # –£–¥–∞–ª—è–µ–º –∫–ª—é—á —Å –ø—Ä–æ–±–µ–ª–æ–º, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ–º
                if key == "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞" and "–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞" not in normalized_result:
                    normalized_result["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = found_doc_type
                    del normalized_result[key]
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ò–ù–ù
        for key in ["–ò–ù–ù_–∑–∞–∫–∞–∑—á–∏–∫–∞", "–ò–ù–ù_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è"]:
            inn = normalized_result.get(key, "")
            if inn and not (inn.isdigit() and len(inn) in [10, 12]):
                inn_matches = re.findall(r'\b\d{10}\b|\b\d{12}\b', original_text)
                if inn_matches:
                    normalized_result[key] = inn_matches.pop(0) if "–∑–∞–∫–∞–∑—á–∏–∫–∞" in key else inn_matches.pop(0) if inn_matches else ""
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ö–ü–ü
        for key in ["–ö–ü–ü_–∑–∞–∫–∞–∑—á–∏–∫–∞", "–ö–ü–ü_–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è"]:
            kpp = normalized_result.get(key, "")
            if kpp and not (kpp.isdigit() and len(kpp) == 9):
                kpp_matches = re.findall(r'\b\d{9}\b', original_text)
                if kpp_matches:
                    normalized_result[key] = kpp_matches.pop(0) if "–∑–∞–∫–∞–∑—á–∏–∫–∞" in key else kpp_matches.pop(0) if kpp_matches else ""
        
        # –û—á–∏—Å—Ç–∫–∞ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_number = normalized_result.get("–ù–æ–º–µ—Ä_–¥–æ–∫—É–º–µ–Ω—Ç–∞", "")
        if doc_number:
            clean_number = re.sub(r'\s*–æ—Ç\s*\d+.*', '', doc_number)
            clean_number = re.sub(r'\s*\d{1,2}[./]\d{1,2}[./]\d{2,4}.*', '', clean_number)
            normalized_result["–ù–æ–º–µ—Ä_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = clean_number.strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        doc_type = normalized_result.get("–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞", "").lower()
        valid_types = ["–¥–æ–≥–æ–≤–æ—Ä", "–∞–∫—Ç", "—Å—á–µ—Ç", "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞"]
        
        if doc_type not in valid_types:
            text_lower = original_text.lower()
            if "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞" in text_lower:
                normalized_result["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞"
            elif "–∞–∫—Ç" in text_lower:
                normalized_result["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–∞–∫—Ç"
            elif "—Å—á–µ—Ç" in text_lower:
                normalized_result["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "—Å—á–µ—Ç"
            elif "–¥–æ–≥–æ–≤–æ—Ä" in text_lower:
                normalized_result["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–¥–æ–≥–æ–≤–æ—Ä"
            else:
                normalized_result["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
        else:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
            normalized_result["–¢–∏–ø_–¥–æ–∫—É–º–µ–Ω—Ç–∞"] = doc_type.lower()
            
        return normalized_result
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        return llm_result


class SuryaSimpleGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("Surya OCR - –ú–∞—Å—Å–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        self.root.geometry("700x600")
        
        # –§–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stop_processing = False
        self.active_processes = []  # –°–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–∫—Ä—ã—Ç–∏—è –æ–∫–Ω–∞
        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        self.pdf_folder = tk.StringVar()
        self.json_folder = tk.StringVar()
        self.date_format = tk.StringVar(value="ISO")
        
        self.processing = False
        self.total_files = 0
        self.processed_files = 0
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM
        self.llm_max_tokens = 16000  # –î–ª—è Phi-4, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.llm_timeout = 180
        self.llm_endpoint = "http://localhost:1234"  # –û–¥–∏–Ω –ø–æ—Ä—Ç –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–í–ï –ú–û–î–ï–õ–ò local-1 –ò local-2
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–≤–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ LM Studio
        self.llm_models = ["local-1", "local-2"]  # –î–≤–µ –º–æ–¥–µ–ª–∏
        self.llm_model_index = 0  # –î–ª—è round-robin
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É—Å–µ—á–µ–Ω–∏—è
        self.first_page_lines = 10   # –ü–µ—Ä–≤—ã–µ N —Å—Ç—Ä–æ–∫ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∑–∞–≥–æ–ª–æ–≤–∫–∏/—Ä–µ–∫–≤–∏–∑–∏—Ç—ã)
        self.last_page_lines = 30    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ N —Å—Ç—Ä–æ–∫ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ–¥–ø–∏—Å–∏/–∏—Ç–æ–≥–∏)
        
        # –ü—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã Surya (–±—É–¥—É—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö)
        self.det_predictor = None
        self.rec_predictor = None
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –ß–∏—Å–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∏–∑ GUI)
        self.ocr_pool_size = 2  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.llm_pool_size = 2  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ OCR
        self.ocr_start_time = 0
        self.ocr_total_time = 0
        self.ocr_doc_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.ocr_completed_count = 0  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.ocr_doc_times = []  # –í—Ä–µ–º–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM
        self.llm_start_time = 0
        self.llm_total_time = 0
        self.llm_doc_count = 0
        self.llm_doc_times = []  # –í—Ä–µ–º–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (—Å—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è OCR + LLM)
        self.total_processing_time = 0
        self.total_doc_count = 0
        
        # –°—á–µ—Ç—á–∏–∫–∏ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.acts_count = 0
        self.invoices_count = 0
        self.bills_count = 0
        self.contracts_count = 0
        
        self.setup_ui()
        
    def setup_ui(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        row = 0
        
        # –í—ã–±–æ—Ä –ø–∞–ø–∫–∏ —Å PDF-—Ñ–∞–π–ª–∞–º–∏
        ttk.Label(main_frame, text="–ü–∞–ø–∫–∞ —Å PDF-—Ñ–∞–π–ª–∞–º–∏:").grid(row=row, column=0, sticky=tk.W, pady=5)
        ttk.Entry(main_frame, textvariable=self.pdf_folder, width=50).grid(row=row, column=1, sticky=(tk.W, tk.E), padx=5)
        ttk.Button(main_frame, text="–û–±–∑–æ—Ä", command=self.select_pdf_folder).grid(row=row, column=2, padx=5)
        row += 1
        
        # –í—ã–±–æ—Ä –ø–∞–ø–∫–∏ –¥–ª—è JSON-—Ñ–∞–π–ª–æ–≤
        ttk.Label(main_frame, text="–ü–∞–ø–∫–∞ –¥–ª—è JSON —Ñ–∞–π–ª–æ–≤:").grid(row=row, column=0, sticky=tk.W, pady=5)
        ttk.Entry(main_frame, textvariable=self.json_folder, width=50).grid(row=row, column=1, sticky=(tk.W, tk.E), padx=5)
        ttk.Button(main_frame, text="–û–±–∑–æ—Ä", command=self.select_json_folder).grid(row=row, column=2, padx=5)
        row += 1
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã
        ttk.Label(main_frame, text="–§–æ—Ä–º–∞—Ç –¥–∞—Ç—ã:").grid(row=row, column=0, sticky=tk.W, pady=5)
        date_frame = ttk.Frame(main_frame)
        date_frame.grid(row=row, column=1, sticky=(tk.W, tk.E), padx=5)
        ttk.Radiobutton(date_frame, text="ISO 8601 (2023-02-17)", variable=self.date_format, value="ISO").pack(side=tk.LEFT)
        ttk.Radiobutton(date_frame, text="–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π (17.02.2023)", variable=self.date_format, value="CLASSIC").pack(side=tk.LEFT, padx=10)
        row += 1
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        ttk.Label(main_frame, text="–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ—Ç–æ–∫–æ–≤:").grid(row=row, column=0, sticky=tk.W, pady=5)
        perf_frame = ttk.Frame(main_frame)
        perf_frame.grid(row=row, column=1, sticky=(tk.W, tk.E), padx=5)
        
        ttk.Label(perf_frame, text="OCR –ø–æ—Ç–æ–∫–æ–≤:").pack(side=tk.LEFT)
        self.ocr_threads_var = tk.StringVar(value="1")
        self.ocr_threads_spinbox = ttk.Spinbox(perf_frame, from_=1, to=8, width=5, textvariable=self.ocr_threads_var)
        self.ocr_threads_spinbox.pack(side=tk.LEFT, padx=(5, 20))
        
        ttk.Label(perf_frame, text="LLM –ø–æ—Ç–æ–∫–æ–≤:").pack(side=tk.LEFT)
        self.llm_threads_var = tk.StringVar(value="1")
        self.llm_threads_spinbox = ttk.Spinbox(perf_frame, from_=1, to=4, width=5, textvariable=self.llm_threads_var)
        self.llm_threads_spinbox.pack(side=tk.LEFT, padx=5)
        row += 1
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≤—Ç–æ–ø–æ–≤—Ç–æ—Ä–∞
        retry_frame = ttk.Frame(main_frame)
        retry_frame.grid(row=row, column=1, sticky=(tk.W, tk.E), padx=5)
        
        self.auto_retry_var = tk.BooleanVar(value=True)
        self.auto_retry_checkbox = ttk.Checkbutton(
            retry_frame, 
            text="–ê–≤—Ç–æ–ø–æ–≤—Ç–æ—Ä –æ—à–∏–±–æ—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–æ 3 –ø–æ–ø—ã—Ç–æ–∫)",
            variable=self.auto_retry_var
        )
        self.auto_retry_checkbox.pack(side=tk.LEFT)
        
        self.max_retries_var = tk.StringVar(value="3")
        ttk.Label(retry_frame, text="–ú–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫:").pack(side=tk.LEFT, padx=(20, 5))
        retry_spinbox = ttk.Spinbox(retry_frame, from_=1, to=5, width=3, textvariable=self.max_retries_var)
        retry_spinbox.pack(side=tk.LEFT)
        row += 1
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM
        llm_frame = ttk.LabelFrame(main_frame, text="–ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM", padding="10")
        llm_frame.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=5)
        
        # –í—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        ttk.Label(llm_frame, text="–ü—Ä–æ–≤–∞–π–¥–µ—Ä:").grid(row=0, column=0, sticky="w", padx=(0, 10))
        self.llm_provider_var = tk.StringVar(value="LM Studio")
        provider_frame = ttk.Frame(llm_frame)
        provider_frame.grid(row=0, column=1, sticky="w")
        ttk.Radiobutton(provider_frame, text="LM Studio", variable=self.llm_provider_var, value="LM Studio", command=self.on_provider_change).pack(side=tk.LEFT)
        ttk.Radiobutton(provider_frame, text="OpenAI", variable=self.llm_provider_var, value="OpenAI", command=self.on_provider_change).pack(side=tk.LEFT, padx=(20, 0))
        
        # OpenAI API –∫–ª—é—á
        ttk.Label(llm_frame, text="OpenAI API –∫–ª—é—á:").grid(row=1, column=0, sticky="w", padx=(0, 10))
        self.openai_api_key_entry = ttk.Entry(llm_frame, width=50, show="*")
        self.openai_api_key_entry.grid(row=1, column=1, sticky="ew")
        
        # –ú–æ–¥–µ–ª—å
        ttk.Label(llm_frame, text="–ú–æ–¥–µ–ª—å:").grid(row=2, column=0, sticky="w", padx=(0, 10))
        self.llm_model_var = tk.StringVar(value="local-1")
        self.llm_model_combobox = ttk.Combobox(llm_frame, textvariable=self.llm_model_var, width=47)
        self.llm_model_combobox['values'] = ('local-1', 'local-2')
        self.llm_model_combobox.grid(row=2, column=1, sticky="ew")
        
        # –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤
        ttk.Label(llm_frame, text="–ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤:").grid(row=3, column=0, sticky="w", padx=(0, 10))
        self.llm_max_tokens_entry = ttk.Entry(llm_frame, width=50)
        self.llm_max_tokens_entry.insert(0, "16000")
        self.llm_max_tokens_entry.grid(row=3, column=1, sticky="ew")
        
        # –¢–∞–π–º–∞—É—Ç
        ttk.Label(llm_frame, text="–¢–∞–π–º–∞—É—Ç (—Å–µ–∫):").grid(row=4, column=0, sticky="w", padx=(0, 10))
        self.llm_timeout_entry = ttk.Entry(llm_frame, width=50)
        self.llm_timeout_entry.insert(0, "180")
        self.llm_timeout_entry.grid(row=4, column=1, sticky="ew")
        
        llm_frame.columnconfigure(1, weight=1)
        row += 1
        
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        button_frame = ttk.Frame(main_frame)
        button_frame.grid(row=row, column=0, columnspan=3, pady=10)
        
        self.start_button = ttk.Button(button_frame, text="–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", command=self.start_processing)
        self.start_button.pack(side=tk.LEFT, padx=5)
        
        self.stop_button = ttk.Button(button_frame, text="–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", command=self.stop_processing_manually, state="disabled")
        self.stop_button.pack(side=tk.LEFT, padx=5)
        row += 1
        
        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        ttk.Label(main_frame, text="–ü—Ä–æ–≥—Ä–µ—Å—Å:").grid(row=row, column=0, sticky=tk.W, pady=5)
        self.progress = ttk.Progressbar(main_frame, mode='determinate')
        self.progress.grid(row=row, column=1, columnspan=2, sticky=(tk.W, tk.E), padx=5)
        row += 1
        
        # –ë–ª–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats_frame = ttk.LabelFrame(main_frame, text="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏", padding="10")
        stats_frame.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=5)
        
        # –õ–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ - OCR —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        ocr_stats_frame = ttk.Frame(stats_frame)
        ocr_stats_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 10))
        ttk.Label(ocr_stats_frame, text="OCR –û–±—Ä–∞–±–æ—Ç–∫–∞:", font=("Arial", 10, "bold")).pack(anchor=tk.W)
        self.ocr_total_time_label = ttk.Label(ocr_stats_frame, text="–û–±—â–µ–µ –≤—Ä–µ–º—è: 0 —Å–µ–∫")
        self.ocr_total_time_label.pack(anchor=tk.W)
        self.ocr_avg_time_label = ttk.Label(ocr_stats_frame, text="–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –¥–æ–∫.: 0 —Å–µ–∫")
        self.ocr_avg_time_label.pack(anchor=tk.W)
        self.ocr_completed_label = ttk.Label(ocr_stats_frame, text="–ó–∞–≤–µ—Ä—à–µ–Ω–æ: 0/0")
        self.ocr_completed_label.pack(anchor=tk.W)
        
        # –ü—Ä–∞–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ - LLM —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        llm_stats_frame = ttk.Frame(stats_frame)
        llm_stats_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        ttk.Label(llm_stats_frame, text="LLM –ê–Ω–∞–ª–∏–∑:", font=("Arial", 10, "bold")).pack(anchor=tk.W)
        self.llm_total_time_label = ttk.Label(llm_stats_frame, text="–û–±—â–µ–µ –≤—Ä–µ–º—è: 0 —Å–µ–∫")
        self.llm_total_time_label.pack(anchor=tk.W)
        self.llm_avg_time_label = ttk.Label(llm_stats_frame, text="–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –¥–æ–∫.: 0 —Å–µ–∫")
        self.llm_avg_time_label.pack(anchor=tk.W)
        self.llm_completed_label = ttk.Label(llm_stats_frame, text="–ó–∞–≤–µ—Ä—à–µ–Ω–æ: 0/0")
        self.llm_completed_label.pack(anchor=tk.W)
        
        # –¢—Ä–µ—Ç—å—è –∫–æ–ª–æ–Ω–∫–∞ - –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_stats_frame = ttk.Frame(stats_frame)
        total_stats_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(10, 0))
        ttk.Label(total_stats_frame, text="–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:", font=("Arial", 10, "bold")).pack(anchor=tk.W)
        self.total_avg_time_label = ttk.Label(total_stats_frame, text="–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –¥–æ–∫.: 0 —Å–µ–∫")
        self.total_avg_time_label.pack(anchor=tk.W)
        self.total_time_breakdown_label = ttk.Label(total_stats_frame, text="OCR + LLM: 0 + 0 —Å")
        self.total_time_breakdown_label.pack(anchor=tk.W)
        self.processing_speed_label = ttk.Label(total_stats_frame, text="–°–∫–æ—Ä–æ—Å—Ç—å: 0 –¥–æ–∫/–º–∏–Ω")
        self.processing_speed_label.pack(anchor=tk.W)
        
        # –ß–µ—Ç–≤–µ—Ä—Ç–∞—è –∫–æ–ª–æ–Ω–∫–∞ - –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_types_frame = ttk.Frame(stats_frame)
        doc_types_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(10, 0))
        ttk.Label(doc_types_frame, text="–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:", font=("Arial", 10, "bold")).pack(anchor=tk.W)
        self.acts_count_label = ttk.Label(doc_types_frame, text="–ê–∫—Ç—ã: 0")
        self.acts_count_label.pack(anchor=tk.W)
        self.invoices_count_label = ttk.Label(doc_types_frame, text="–°—á–µ—Ç–∞: 0")
        self.invoices_count_label.pack(anchor=tk.W)
        self.bills_count_label = ttk.Label(doc_types_frame, text="–°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä—ã: 0")
        self.bills_count_label.pack(anchor=tk.W)
        self.contracts_count_label = ttk.Label(doc_types_frame, text="–î–æ–≥–æ–≤–æ—Ä—ã: 0")
        self.contracts_count_label.pack(anchor=tk.W)
        
        row += 1
        
        # –õ–æ–≥
        ttk.Label(main_frame, text="–õ–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∫–∏:").grid(row=row, column=0, sticky=tk.W, pady=5)
        row += 1
        
        self.log_text = scrolledtext.ScrolledText(main_frame, height=20, width=80)
        self.log_text.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        row += 1
        
        # –ö–Ω–æ–ø–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∞
        ttk.Button(main_frame, text="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥ –∫–∞–∫ txt —Ñ–∞–π–ª", command=self.save_log).grid(row=row, column=0, columnspan=3, pady=10)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Ç–∫–∏
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(row-2, weight=1)
        
        # –ù–∞—á–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        self.on_provider_change()
        
    def select_pdf_folder(self):
        folder = filedialog.askdirectory()
        if folder:
            self.pdf_folder.set(folder)
            
    def select_json_folder(self):
        folder = filedialog.askdirectory()
        if folder:
            self.json_folder.set(folder)
            
    def on_provider_change(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ–Ω—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LLM"""
        provider = self.llm_provider_var.get()
        
        if provider == "OpenAI":
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è OpenAI
            self.openai_api_key_entry.config(state="normal")
            self.llm_model_combobox['values'] = (
                'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo',
                'o1-preview', 'o1-mini'
            )
            self.llm_model_var.set('gpt-4o-mini')
            self.llm_max_tokens_entry.delete(0, tk.END)
            self.llm_max_tokens_entry.insert(0, "16000")
        else:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è LM Studio
            self.openai_api_key_entry.config(state="disabled")
            self.llm_model_combobox['values'] = ('local-1', 'local-2')
            self.llm_model_var.set('local-1')
            self.llm_max_tokens_entry.delete(0, tk.END)
            self.llm_max_tokens_entry.insert(0, "16000")
    
    def on_closing(self):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–∫—Ä—ã—Ç–∏—è –æ–∫–Ω–∞ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã"""
        self.log("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–∫—Ä—ã—Ç–∏—è - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã...")
        self.stop_processing = True
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
        for process in self.active_processes:
            if process.is_alive():
                self.log(f"–ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å PID: {process.pid}")
                process.terminate()
                process.join(timeout=2)
                if process.is_alive():
                    process.kill()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        
        self.active_processes.clear()
        self.root.destroy()
    
    def stop_processing_manually(self):
        """–†—É—á–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.log("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Å—Ç–∞–Ω–æ–≤–∏–ª –æ–±—Ä–∞–±–æ—Ç–∫—É")
        self.stop_processing = True
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
        for process in self.active_processes:
            if process.is_alive():
                self.log(f"–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å PID: {process.pid}")
                process.terminate()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–Ω–æ–ø–∫–∏ –≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.start_button.config(state="normal")
        self.stop_button.config(state="disabled")
            
    def log(self, message):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ª–æ–≥"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        self.log_text.insert(tk.END, log_message + "\n")
        self.log_text.see(tk.END)
        self.root.update_idletasks()
        
    def update_progress(self, current, total):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞"""
        if total > 0:
            progress_value = (current / total) * 100
            self.progress['value'] = progress_value
        self.root.update_idletasks()
            
    def update_ocr_stats(self, completed_count, total_count, doc_time=None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ OCR"""
        self.ocr_completed_count = completed_count
        self.ocr_doc_count = completed_count  # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if doc_time:
            self.ocr_doc_times.append(doc_time)
            
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è
        if self.ocr_start_time > 0:
            self.ocr_total_time = time.time() - self.ocr_start_time
            
        # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
        avg_time = sum(self.ocr_doc_times) / len(self.ocr_doc_times) if self.ocr_doc_times else 0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º GUI
        self.ocr_total_time_label.config(text=f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {self.ocr_total_time:.1f} —Å–µ–∫")
        self.ocr_avg_time_label.config(text=f"–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –¥–æ–∫.: {avg_time:.1f} —Å–µ–∫")
        self.ocr_completed_label.config(text=f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {completed_count}/{total_count}")
        self.root.update_idletasks()
        
    def update_llm_stats(self, completed_count, total_count, doc_time=None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ LLM"""
        self.llm_doc_count = completed_count  # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if doc_time:
            self.llm_doc_times.append(doc_time)
            
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è
        if self.llm_start_time > 0:
            self.llm_total_time = time.time() - self.llm_start_time
            
        # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
        avg_time = sum(self.llm_doc_times) / len(self.llm_doc_times) if self.llm_doc_times else 0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º GUI
        self.llm_total_time_label.config(text=f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {self.llm_total_time:.1f} —Å–µ–∫")
        self.llm_avg_time_label.config(text=f"–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –¥–æ–∫.: {avg_time:.1f} —Å–µ–∫")
        self.llm_completed_label.config(text=f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {completed_count}/{total_count}")
        self.root.update_idletasks()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.update_total_stats()
    
    def update_document_type_count(self, doc_type):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–æ–≤ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–∏–ø –Ω–µ –ø—É—Å—Ç–æ–π
        if doc_type:
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            doc_type_lower = doc_type.lower()
            
            if doc_type_lower == "–∞–∫—Ç":
                self.acts_count += 1
            elif doc_type_lower in ["—Å—á—ë—Ç", "—Å—á–µ—Ç"]:
                self.invoices_count += 1
            elif doc_type_lower == "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞":
                self.bills_count += 1
            elif doc_type_lower == "–¥–æ–≥–æ–≤–æ—Ä":
                self.contracts_count += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º GUI
        self.acts_count_label.config(text=f"–ê–∫—Ç—ã: {self.acts_count}")
        self.invoices_count_label.config(text=f"–°—á–µ—Ç–∞: {self.invoices_count}")
        self.bills_count_label.config(text=f"–°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä—ã: {self.bills_count}")
        self.contracts_count_label.config(text=f"–î–æ–≥–æ–≤–æ—Ä—ã: {self.contracts_count}")
        self.root.update_idletasks()
    
    def update_total_stats(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (OCR + LLM)"""
        if self.ocr_doc_count > 0 and self.llm_doc_count > 0:
            # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è OCR –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
            avg_ocr_time = self.ocr_total_time / self.ocr_doc_count
            # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è LLM –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
            avg_llm_time = self.llm_total_time / self.llm_doc_count
            # –û–±—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
            total_avg_time = avg_ocr_time + avg_llm_time
            
            # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
            docs_per_minute = 60 / total_avg_time if total_avg_time > 0 else 0
            
            self.total_avg_time_label.config(text=f"–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –¥–æ–∫.: {total_avg_time:.1f} —Å–µ–∫")
            self.total_time_breakdown_label.config(text=f"OCR + LLM: {avg_ocr_time:.1f} + {avg_llm_time:.1f} —Å")
            self.processing_speed_label.config(text=f"–°–∫–æ—Ä–æ—Å—Ç—å: {docs_per_minute:.1f} –¥–æ–∫/–º–∏–Ω")
        
        self.root.update_idletasks()
        
    def initialize_surya(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ Surya (–≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ)"""
        self.det_predictor = DetectionPredictor()
        self.rec_predictor = RecognitionPredictor()
        
    def process_pdf_with_surya(self, pdf_path):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Å Surya OCR, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º"""
        try:
            images, names = load_from_file(pdf_path)
            task_names = [TaskNames.ocr_with_boxes] * len(images)
            predictions = self.rec_predictor(
                images,
                task_names=task_names,
                det_predictor=self.det_predictor,
                math_mode=False
            )
            
            pages_data = []
            combined_text = ""
            
            for page_idx, pred in enumerate(predictions):
                page_lines = []
                page_text = ""
                for line in pred.text_lines:
                    line_data = {
                        "text": line.text,
                        "bbox": line.bbox,
                        "confidence": line.confidence
                    }
                    page_lines.append(line_data)
                    page_text += line.text + " "
                pages_data.append({
                    "page": page_idx + 1,
                    "text_lines": page_lines
                })
                combined_text += page_text
            
            combined_json = {
                "filename": os.path.basename(pdf_path),
                "pages": len(predictions),
                "pages_data": pages_data,
                "full_text": combined_text.strip()
            }
            
            return combined_json, combined_text.strip()
            
        except Exception as e:
            return None, None
            
    def prepare_truncated_ocr_for_llm(self, ocr_json_data):
        """–£–º–Ω–æ–µ —É—Å–µ—á–µ–Ω–∏–µ OCR –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º —Ç–æ–∫–µ–Ω–æ–≤"""
        pages_data = ocr_json_data.get("pages_data", [])
        total_pages = len(pages_data)
        
        if total_pages == 0:
            return []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        all_lines = []
        for page_idx, page in enumerate(pages_data):
            for line in page["text_lines"]:
                all_lines.append({
                    "page": page_idx + 1,
                    "text": line["text"],
                    "bbox": line["bbox"],
                    "confidence": line["confidence"]
                })
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –ª–æ–≥–∏–∫—É —É—Å–µ—á–µ–Ω–∏—è —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º —Ç–æ–∫–µ–Ω–æ–≤
        max_tokens = 12000  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (4000 —Ç–æ–∫–µ–Ω–æ–≤)
        truncated_lines, token_count, was_truncated = smart_truncate_for_llm(all_lines, max_tokens)
        
        if was_truncated:
            self.log(f"‚úÇÔ∏è –î–æ–∫—É–º–µ–Ω—Ç —É—Å–µ—á–µ–Ω: {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
        else:
            self.log(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –ø–æ–º–µ—â–∞–µ—Ç—Å—è: {token_count} —Ç–æ–∫–µ–Ω–æ–≤")
        
        return truncated_lines
        
    def save_to_csv(self, filename, ocr_json, ocr_text, pdf_folder):
        csv_file = os.path.join(os.path.dirname(pdf_folder), "ocr_result.csv")
        
        file_exists = os.path.exists(csv_file)
        
        try:
            with open(csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                if not file_exists:
                    writer.writerow(['filename', 'recognition_date', 'ocr_json', 'ocr_text'])
                
                date_str = datetime.now().strftime("%Y-%m-%d" if self.date_format.get() == "ISO" else "%d.%m.%Y")
                
                writer.writerow([
                    filename,
                    date_str,
                    json.dumps(ocr_json, ensure_ascii=False),
                    ocr_text
                ])
        except Exception as e:
            pass  # –õ–æ–≥ –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ
            
    def analyze_with_llm(self, filename, truncated_data):
        """–ê–Ω–∞–ª–∏–∑ —É—Å–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å LLM (–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ –∏–º–µ–Ω–∞–º –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –ø–æ—Ä—Ç—É)"""
        try:
            structured_data = json.dumps(truncated_data, ensure_ascii=False, indent=2)
            
            prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–µ–ª–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ö —Ç–µ–±–µ –ø—Ä–∏—Ö–æ–¥—è—Ç –£–°–ï–ß–ï–ù–ù–´–ï –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ï –¥–∞–Ω–Ω—ã–µ –æ—Ç Surya OCR —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ (bbox) –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏. –§–æ–∫—É—Å –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —á–∞—Å—Ç—è—Ö: –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ä–µ–∫–≤–∏–∑–∏—Ç—ã, —Ç–∏–ø, –Ω–æ–º–µ—Ä, –¥–∞—Ç–∞, —Å—Ç–æ—Ä–æ–Ω—ã) –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ (–∞–¥—Ä–µ—Å–∞, –ø–æ–¥–ø–∏—Å–∏, –ò–ù–ù, –ö–ü–ü, –∏—Ç–æ–≥–∏).

–ö–û–û–†–î–ò–ù–ê–¢–´ –ø–æ–º–æ–≥–∞—é—Ç –ø–æ–Ω—è—Ç—å –†–ê–°–ü–û–õ–û–ñ–ï–ù–ò–ï:
- –ú–∞–ª—ã–µ X = –ª–µ–≤–∞—è —á–∞—Å—Ç—å, –±–æ–ª—å—à–∏–µ X = –ø—Ä–∞–≤–∞—è —á–∞—Å—Ç—å
- –ú–∞–ª—ã–µ Y = –≤–µ—Ä—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –±–æ–ª—å—à–∏–µ Y = –Ω–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û! –õ–û–ì–ò–ö–ê –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –†–û–õ–ï–ô:
1. –ê–ö–¢ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç/—É—Å–ª—É–≥:
   - –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨ = —Ç–æ—Ç, –∫—Ç–æ –í–´–ü–û–õ–ù–ò–õ —Ä–∞–±–æ—Ç—ã (–æ–±—ã—á–Ω–æ —Å–ª–µ–≤–∞ –∏–ª–∏ –ø–æ–¥–ø–∏—Å—ã–≤–∞–µ—Ç –∞–∫—Ç)
   - –ó–ê–ö–ê–ó–ß–ò–ö = —Ç–æ—Ç, –∫—Ç–æ –ü–†–ò–ù–ò–ú–ê–ï–¢ —Ä–∞–±–æ—Ç—ã (–æ–±—ã—á–Ω–æ —Å–ø—Ä–∞–≤–∞)
   - –ü—Ä–∏–º–µ—Ä: "–ê–≤—Ç–æ–∞—Å—Å–∏—Å—Ç–∞–Ω—Å" –≤—ã–ø–æ–ª–Ω–∏–ª —É—Å–ª—É–≥–∏ –¥–ª—è "–î–µ–∞–ª–æ–Ω"

2. –°–ß–ï–¢ –Ω–∞ –æ–ø–ª–∞—Ç—É:
   - –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨ = –ø–æ—Å—Ç–∞–≤—â–∏–∫, –∫—Ç–æ –í–´–°–¢–ê–í–õ–Ø–ï–¢ —Å—á–µ—Ç (–ø–æ–ª—É—á–∞—Ç–µ–ª—å –¥–µ–Ω–µ–≥)
   - –ó–ê–ö–ê–ó–ß–ò–ö = –ø–ª–∞—Ç–µ–ª—å—â–∏–∫, –∫–æ–º—É –≤—ã—Å—Ç–∞–≤–ª–µ–Ω —Å—á–µ—Ç (–ø–ª–∞—Ç–∏—Ç –¥–µ–Ω—å–≥–∏)

3. –°–ß–ï–¢-–§–ê–ö–¢–£–†–ê:
   - –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨ = –ø—Ä–æ–¥–∞–≤–µ—Ü, –∫—Ç–æ –ü–û–°–¢–ê–í–õ–Ø–ï–¢ —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏
   - –ó–ê–ö–ê–ó–ß–ò–ö = –ø–æ–∫—É–ø–∞—Ç–µ–ª—å, –∫—Ç–æ –ü–û–õ–£–ß–ê–ï–¢ —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏

4. –î–û–ì–û–í–û–†:
   - –°–º–æ—Ç—Ä–∏ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç: –∫—Ç–æ –ø–æ—Å—Ç–∞–≤—â–∏–∫, –∫—Ç–æ –∑–∞–∫–∞–∑—á–∏–∫

–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê –î–û–ö–£–ú–ï–ù–¢–ê:
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å "–ê–ö–¢" - —ç—Ç–æ –ê–∫—Ç
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å "–°–ß–ï–¢" (–Ω–æ –Ω–µ "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞") - —ç—Ç–æ –°—á—ë—Ç
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å "–°–ß–ï–¢-–§–ê–ö–¢–£–†–ê" - —ç—Ç–æ –°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å "–î–û–ì–û–í–û–†" - —ç—Ç–æ –î–æ–≥–æ–≤–æ—Ä

–ü–†–ê–í–ò–õ–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø:
1. –ò–ù–ù: –¢–û–õ–¨–ö–û 10 –∏–ª–∏ 12 —Ü–∏—Ñ—Ä (–Ω–µ –ø—É—Ç–∞–π —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—á–µ—Ç–æ–≤!)
2. –ö–ü–ü: –¢–û–õ–¨–ö–û 9 —Ü–∏—Ñ—Ä
3. –ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞: –±–µ–∑ "–æ—Ç", "‚Ññ", –¥–∞—Ç - —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã/–±—É–∫–≤—ã
4. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï SURYA OCR (—É—Å–µ—á–µ–Ω–Ω—ã–µ):
{structured_data}

–ê–ù–ê–õ–ò–ó–ò–†–£–ô –ü–û–®–ê–ì–û–í–û:
1. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ñ–æ–∫—É—Å –Ω–∞ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö)
2. –ù–∞–π–¥–∏ –Ω–æ–º–µ—Ä –∏ –¥–∞—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ–±—ã—á–Ω–æ –≤ –ø–µ—Ä–≤—ã—Ö)
3. –û–ø—Ä–µ–¥–µ–ª–∏, –∫—Ç–æ –ó–ê–ö–ê–ó–ß–ò–ö (–ø–æ–ª—É—á–∞–µ—Ç —É—Å–ª—É–≥–∏), –∫—Ç–æ –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨ (–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å–ª—É–≥–∏)
4. –ò–∑–≤–ª–µ–∫–∏ –ò–ù–ù, –ö–ü–ü, –∞–¥—Ä–µ—Å–∞ –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã (—á–∞—Å—Ç–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–ª—è –ø–æ–¥–ø–∏—Å–µ–π)
5. –ü—Ä–æ–≤–µ—Ä—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ò–ù–ù (10-12 —Ü–∏—Ñ—Ä) –∏ –ö–ü–ü (9 —Ü–∏—Ñ—Ä)

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¢–û–õ–¨–ö–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{
  "–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞": "{filename}",
  "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞": "–ê–∫—Ç|–°—á—ë—Ç|–°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞|–î–æ–≥–æ–≤–æ—Ä" (–≤—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ!),
  "–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞": "",
  "–î–∞—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞": "",
  "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞": "",
  "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "",
  "–ò–ù–ù –∑–∞–∫–∞–∑—á–∏–∫–∞": "",
  "–ò–ù–ù –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "",
  "–ö–ü–ü –∑–∞–∫–∞–∑—á–∏–∫–∞": "",
  "–ö–ü–ü –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": "",
  "–ê–¥—Ä–µ—Å –∑–∞–∫–∞–∑—á–∏–∫–∞": "",
  "–ê–¥—Ä–µ—Å –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è": ""
}}"""

            # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
            model = self.llm_models[self.llm_model_index % len(self.llm_models)]
            self.llm_model_index += 1
            
            headers = {"Content-Type": "application/json"}
            
            data = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1,
                "max_tokens": self.llm_max_tokens
            }
            
            response = requests.post(f"{self.llm_endpoint}/v1/chat/completions", 
                                     headers=headers, json=data, timeout=self.llm_timeout)
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end != -1:
                    json_str = content[start:end]
                    return json.loads(json_str)
                
            return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON"}
            
        except Exception as e:
            return {"error": str(e)}
            
    def validate_and_fix_llm_result(self, llm_result, original_text):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)"""
        if "error" in llm_result:
            return llm_result
            
        try:
            import re
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ò–ù–ù
            for key in ["–ò–ù–ù –∑–∞–∫–∞–∑—á–∏–∫–∞", "–ò–ù–ù –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è"]:
                inn = llm_result.get(key, "")
                if inn and not (inn.isdigit() and len(inn) in [10, 12]):
                    inn_matches = re.findall(r'\b\d{10}\b|\b\d{12}\b', original_text)
                    if inn_matches:
                        llm_result[key] = inn_matches.pop(0) if "–∑–∞–∫–∞–∑—á–∏–∫–∞" in key else inn_matches.pop(0) if inn_matches else ""
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ö–ü–ü
            for key in ["–ö–ü–ü –∑–∞–∫–∞–∑—á–∏–∫–∞", "–ö–ü–ü –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è"]:
                kpp = llm_result.get(key, "")
                if kpp and not (kpp.isdigit() and len(kpp) == 9):
                    kpp_matches = re.findall(r'\b\d{9}\b', original_text)
                    if kpp_matches:
                        llm_result[key] = kpp_matches.pop(0) if "–∑–∞–∫–∞–∑—á–∏–∫–∞" in key else kpp_matches.pop(0) if kpp_matches else ""
            
            # –û—á–∏—Å—Ç–∫–∞ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_number = llm_result.get("–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞", "")
            if doc_number:
                clean_number = re.sub(r'\s*–æ—Ç\s*\d+.*', '', doc_number)
                clean_number = re.sub(r'\s*\d{1,2}[./]\d{1,2}[./]\d{2,4}.*', '', clean_number)
                llm_result["–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞"] = clean_number.strip()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_type = llm_result.get("–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞", "")
            valid_types = ["–ê–∫—Ç", "–°—á—ë—Ç", "–°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞", "–î–æ–≥–æ–≤–æ—Ä"]
            if doc_type not in valid_types:
                text_lower = original_text.lower()
                if "—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞" in text_lower:
                    llm_result["–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞"
                elif "–∞–∫—Ç" in text_lower:
                    llm_result["–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–ê–∫—Ç"
                elif "—Å—á–µ—Ç" in text_lower:
                    llm_result["–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–°—á—ë—Ç"
                elif "–¥–æ–≥–æ–≤–æ—Ä" in text_lower:
                    llm_result["–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–î–æ–≥–æ–≤–æ—Ä"
                else:
                    llm_result["–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"] = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
            
            return llm_result
            
        except Exception as e:
            return llm_result
            

    
    def save_llm_result(self, filename, llm_result, json_folder):
        try:
            json_filename = os.path.splitext(filename)[0] + ".json"
            json_path = os.path.join(json_folder, json_filename)
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(llm_result, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            pass  # –õ–æ–≥ –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ
            
    def process_files(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å multiprocessing"""
        try:
            pdf_folder = self.pdf_folder.get()
            json_folder = self.json_folder.get()
            
            if not pdf_folder or not json_folder:
                self.log("–û—à–∏–±–∫–∞: –ù–µ –≤—ã–±—Ä–∞–Ω—ã –ø–∞–ø–∫–∏")
                return
                
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ—Ç–æ–∫–æ–≤ –∏–∑ GUI
            self.ocr_pool_size = int(self.ocr_threads_var.get())
            self.llm_pool_size = int(self.llm_threads_var.get())
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.ocr_start_time = 0
            self.ocr_total_time = 0
            self.ocr_completed_count = 0
            self.ocr_doc_times = []
            self.llm_start_time = 0
            self.llm_total_time = 0
            self.llm_completed_count = 0
            self.llm_doc_times = []
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self.acts_count = 0
            self.invoices_count = 0
            self.bills_count = 0
            self.contracts_count = 0
            self.update_document_type_count("")  # –û–±–Ω–æ–≤–ª—è–µ–º GUI
            
            os.makedirs(json_folder, exist_ok=True)
            
            pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith('.pdf')]
            
            if not pdf_files:
                messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–ù–µ—Ç PDF —Ñ–∞–π–ª–æ–≤")
                return
                
            self.total_files = len(pdf_files)
            self.processed_files = 0
            
            start_time = datetime.now()
            self.log(f"=== –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò ===")
            self.log(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            self.log(f"–ù–∞–π–¥–µ–Ω–æ {self.total_files} PDF —Ñ–∞–π–ª–æ–≤")
            self.log(f"OCR –ø–æ—Ç–æ–∫–æ–≤: {self.ocr_pool_size}, LLM –º–æ–¥–µ–ª–µ–π: {self.llm_pool_size}")
            
            # –≠–¢–ê–ü 1: OCR –û–ë–†–ê–ë–û–¢–ö–ê —Å –æ—á–µ—Ä–µ–¥—è–º–∏ (–ù–ï–ë–õ–û–ö–ò–†–£–Æ–©–ê–Ø!)
            self.log(f"–≠–¢–ê–ü 1: –ù–∞—á–∏–Ω–∞–µ–º OCR –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ {self.ocr_pool_size} –ø–æ—Ç–æ–∫–∞...")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä OCR
            self.ocr_start_time = time.time()
            self.update_ocr_stats(0, len(pdf_files))
            
            # –û—á–µ—Ä–µ–¥–∏ –¥–ª—è OCR
            pdf_queue = Queue()
            ocr_result_queue = Queue()
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å PDF
            for pdf_file in pdf_files:
                pdf_queue.put((pdf_file, pdf_folder, self.date_format.get()))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if self.stop_processing:
                self.log("–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                return
            
            # –ó–∞–ø—É—Å–∫ OCR –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            ocr_processes = []
            for i in range(self.ocr_pool_size):
                p = Process(target=ocr_worker_simple, args=(pdf_queue, ocr_result_queue))
                p.start()
                ocr_processes.append(p)
                self.active_processes.append(p)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            ocr_completed = 0
            ocr_data_list = []
            
            while ocr_completed < len(pdf_files) and not self.stop_processing:
                try:
                    result = ocr_result_queue.get(timeout=1)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    if self.stop_processing:
                        self.log("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
                        break
                    
                    doc_time = result.get('processing_time', 0)
                    
                    if result["success"]:
                        ocr_data_list.append(result)
                        ocr_completed += 1
                        self.log(f"OCR –∑–∞–≤–µ—Ä—à–µ–Ω: {result['filename']} ({doc_time:.1f}—Å)")
                        self.update_progress(ocr_completed, len(pdf_files))
                        self.update_ocr_stats(ocr_completed, len(pdf_files), doc_time)
                    else:
                        ocr_completed += 1
                        self.log(f"OCR –æ—à–∏–±–∫–∞: {result['filename']} - {result['error']}")
                        self.update_progress(ocr_completed, len(pdf_files))
                        self.update_ocr_stats(ocr_completed, len(pdf_files))
                except queue.Empty:
                    self.root.update()  # –û–±–Ω–æ–≤–ª—è–µ–º GUI
                    continue
            
            # –ó–∞–≤–µ—Ä—à–∞–µ–º OCR –ø—Ä–æ—Ü–µ—Å—Å—ã
            for _ in range(self.ocr_pool_size):
                pdf_queue.put(None)
            for p in ocr_processes:
                p.join()
            
            self.log(f"–≠–¢–ê–ü 1 –ó–ê–í–ï–†–®–ï–ù: OCR –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {ocr_completed}/{len(pdf_files)} —Ñ–∞–π–ª–æ–≤")
            
            if not ocr_data_list:
                messagebox.showerror("–û—à–∏–±–∫–∞", "OCR –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
                return
            
            # –≠–¢–ê–ü 2: LLM –û–ë–†–ê–ë–û–¢–ö–ê (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ local-1 –∏ local-2)
            self.log(f"–≠–¢–ê–ü 2: –ù–∞—á–∏–Ω–∞–µ–º LLM –∞–Ω–∞–ª–∏–∑ —Å {self.llm_pool_size} –≤–æ—Ä–∫–µ—Ä–∞–º–∏...")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä LLM
            self.llm_start_time = time.time()
            self.update_llm_stats(0, len(ocr_data_list))
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –∏ –∞–≤—Ç–æ–ø–æ–≤—Ç–æ—Ä–∞ –∏–∑ GUI
            provider = self.llm_provider_var.get()
            auto_retry = self.auto_retry_var.get()
            max_retries = int(self.max_retries_var.get())
            
            llm_settings = {
                'provider': provider,
                'max_tokens': int(self.llm_max_tokens_entry.get()),
                'timeout': int(self.llm_timeout_entry.get()),
                'endpoint': self.llm_endpoint,
                'auto_retry': auto_retry,
                'max_retries': max_retries
            }
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤
            if provider == 'OpenAI':
                api_key = self.openai_api_key_entry.get().strip()
                if not api_key:
                    messagebox.showerror("–û—à–∏–±–∫–∞", "–í–≤–µ–¥–∏—Ç–µ OpenAI API –∫–ª—é—á!")
                    return
                llm_settings['api_key'] = api_key
                # –î–ª—è OpenAI –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –º–æ–¥–µ–ª—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–æ—Ä–∫–µ—Ä–∞–º–∏
                model_name = self.llm_model_var.get()
                self.llm_models = [model_name] * self.llm_pool_size  # –î—É–±–ª–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ—Ç–æ–∫–æ–≤
            else:
                # –î–ª—è LM Studio —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ—Ç–æ–∫–æ–≤
                if self.llm_pool_size == 1:
                    self.llm_models = ["local-model"]  # –û–¥–∏–Ω –≤–æ—Ä–∫–µ—Ä
                elif self.llm_pool_size == 2:
                    self.llm_models = ["local-1", "local-2"]  # –î–≤–∞ –≤–æ—Ä–∫–µ—Ä–∞
                else:
                    # –ë–æ–ª—å—à–µ 2 –ø–æ—Ç–æ–∫–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –º–æ–¥–µ–ª—å
                    self.llm_models = ["local-model"] * self.llm_pool_size
            
            # –û—á–µ—Ä–µ–¥–∏ –¥–ª—è LLM –∏ –ø–æ–≤—Ç–æ—Ä–æ–≤
            llm_queue = Queue()
            retry_queue = Queue() if auto_retry else None
            result_queue = Queue()
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å OCR –¥–∞–Ω–Ω—ã–º–∏
            for ocr_data in ocr_data_list:
                llm_queue.put((ocr_data['filename'], ocr_data['truncated_data'], ocr_data['combined_text']))
            
            # –ó–∞–ø—É—Å–∫ LLM –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            self.log(f"–°–æ–∑–¥–∞–µ–º {len(self.llm_models)} LLM –≤–æ—Ä–∫–µ—Ä–æ–≤: {self.llm_models}")
            llm_processes = []
            for i, model in enumerate(self.llm_models):
                worker_name = f"LLM-{i+1}" if len(self.llm_models) > 1 else "LLM"
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                if self.stop_processing:
                    self.log("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º LLM")
                    return
                    
                self.log(f"–ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ—Ä–∫–µ—Ä: {worker_name} (–º–æ–¥–µ–ª—å: {model})")
                p = Process(target=llm_worker, args=(llm_queue, result_queue, json_folder, llm_settings, model, worker_name, retry_queue))
                p.start()
                llm_processes.append(p)
                self.active_processes.append(p)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–≤—Ç–æ—Ä–æ–≤
            llm_completed = 0
            retry_added = 0  # –°—á–µ—Ç—á–∏–∫ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤
            
            while llm_completed < len(ocr_data_list) and not self.stop_processing:
                try:
                    result = result_queue.get(timeout=1)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    if self.stop_processing:
                        self.log("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
                        break
                    
                    self.log(result)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä—ã –≤ –æ—á–µ—Ä–µ–¥–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –≤ –æ—Å–Ω–æ–≤–Ω—É—é –æ—á–µ—Ä–µ–¥—å
                    if auto_retry and retry_queue:
                        while not retry_queue.empty():
                            try:
                                retry_item = retry_queue.get_nowait()
                                llm_queue.put(retry_item)
                                retry_added += 1
                                self.log(f"–î–æ–±–∞–≤–ª–µ–Ω –ø–æ–≤—Ç–æ—Ä –≤ –æ—á–µ—Ä–µ–¥—å: {retry_item[0]}")
                            except queue.Empty:
                                break
                    
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¢–û–õ–¨–ö–û –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–µ –ø–æ–≤—Ç–æ—Ä–æ–≤)
                    if "–ó–∞–≤–µ—Ä—à–µ–Ω–æ" in result or ("–û—à–∏–±–∫–∞ LLM" in result and "–ø–æ–ø—ã—Ç–∫–∞" in result):
                        # –ü–∞—Ä—Å–∏–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        doc_time = 0
                        if "(–≤—Ä–µ–º—è:" in result:
                            try:
                                time_part = result.split("(–≤—Ä–µ–º—è: ")[1].split("—Å)")[0]
                                doc_time = float(time_part)
                            except:
                                pass
                        
                        # –ü–∞—Ä—Å–∏–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        if "–ó–∞–≤–µ—Ä—à–µ–Ω–æ" in result and " - " in result:
                            try:
                                doc_type = result.split(" - ")[-1].strip()
                                if doc_type and doc_type != "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω":
                                    self.update_document_type_count(doc_type)
                            except:
                                pass
                        
                        llm_completed += 1
                        self.update_progress(llm_completed, len(ocr_data_list))
                        self.update_llm_stats(llm_completed, len(ocr_data_list), doc_time if doc_time > 0 else None)
                        
                except queue.Empty:
                    self.root.update()  # –û–±–Ω–æ–≤–ª—è–µ–º GUI
                    continue
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø–æ–≤—Ç–æ—Ä–æ–≤ –ø–µ—Ä–µ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º
            if auto_retry and retry_queue:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø–æ–≤—Ç–æ—Ä—ã
                final_retries = 0
                while not retry_queue.empty():
                    try:
                        retry_item = retry_queue.get_nowait()
                        llm_queue.put(retry_item)
                        final_retries += 1
                        self.log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –ø–æ–≤—Ç–æ—Ä–∞: {retry_item[0]}")
                    except queue.Empty:
                        break
                        
                if final_retries > 0:
                    self.log(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {final_retries} –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø–æ–≤—Ç–æ—Ä–æ–≤...")
                    
                    # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤
                    final_completed = 0
                    while final_completed < final_retries:
                        try:
                            result = result_queue.get(timeout=5)
                            self.log(result)
                            if "–ó–∞–≤–µ—Ä—à–µ–Ω–æ" in result or "–û—à–∏–±–∫–∞ LLM" in result:
                                final_completed += 1
                        except queue.Empty:
                            self.log("–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤")
                            break
            
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ LLM –≤–æ—Ä–∫–µ—Ä–æ–≤
            for _ in range(len(self.llm_models)):
                llm_queue.put(None)
            
            for p in llm_processes:
                p.join()
            
            self.processed_files = llm_completed
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            self.log(f"\n=== –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê ===")
            self.log(f"–í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            self.log(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration}")
            self.log(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.processed_files}/{self.total_files}")
            if auto_retry and retry_added > 0:
                self.log(f"–ü–æ–≤—Ç–æ—Ä–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {retry_added}")
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–Ω–æ–ø–æ–∫
            self.start_button.config(state="normal")
            self.stop_button.config(state="disabled")
            
            # –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            self.active_processes.clear()
            
            if not self.stop_processing:
                messagebox.showinfo("–ì–æ—Ç–æ–≤–æ", f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.processed_files}/{self.total_files}")
            else:
                messagebox.showwarning("–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ", "–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            
        except Exception as e:
            self.log(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            messagebox.showerror("–û—à–∏–±–∫–∞", str(e))
        finally:
            self.processing = False
            self.start_button.config(text="–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", state="normal")
            
    def start_processing(self):
        if self.processing:
            self.processing = False
        
        # –ú–µ–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–Ω–æ–ø–æ–∫
        self.start_button.config(state="disabled")
        self.stop_button.config(state="normal")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        processing_thread = threading.Thread(target=self.process_files, daemon=True)
        processing_thread.start()

    def save_log(self):
        try:
            log_content = self.log_text.get(1.0, tk.END)
            filename = f"surya_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(log_content)
                
            messagebox.showinfo("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ", f"–õ–æ–≥ –≤: {filename}")
        except Exception as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", str(e))


def main():
    root = tk.Tk()
    app = SuryaSimpleGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()