#!/usr/bin/env python3
"""
–¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç—á–∏–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è OCR –¥–∞–Ω–Ω—ã—Ö
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç tiktoken –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
"""

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    print("‚ö†Ô∏è tiktoken –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–Ω–∫–æ–¥–µ—Ä —Ç–æ–∫–µ–Ω–æ–≤
if TIKTOKEN_AVAILABLE:
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º cl100k_base - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è GPT-4 –∏ –ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        TOKEN_ENCODER = tiktoken.get_encoding("cl100k_base")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ tiktoken: {e}")
        TIKTOKEN_AVAILABLE = False
        TOKEN_ENCODER = None
else:
    TOKEN_ENCODER = None

def estimate_tokens(text):
    """
    –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é tiktoken
    –ï—Å–ª–∏ tiktoken –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
    """
    if not text:
        return 0
    
    if TIKTOKEN_AVAILABLE and TOKEN_ENCODER:
        try:
            # –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å tiktoken
            tokens = TOKEN_ENCODER.encode(text)
            return len(tokens)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ tiktoken, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç: {e}")
    
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    char_count = len(text)
    estimated_tokens = char_count / 4.2  # 1 —Ç–æ–∫–µ–Ω ‚âà 4.2 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    return int(estimated_tokens)

def estimate_tokens_from_lines(lines_data):
    """
    –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö OCR
    """
    if not lines_data:
        return 0
    
    total_text = ""
    for line in lines_data:
        if isinstance(line, dict) and 'text' in line:
            total_text += line['text'] + " "
        elif isinstance(line, str):
            total_text += line + " "
    
    return estimate_tokens(total_text)

def check_context_limit(lines_data, max_tokens=16000):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—Ä–µ–≤—ã—à–∞–µ—Ç_–ª–∏–º–∏—Ç, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Ç–æ–∫–µ–Ω–æ–≤)
    """
    token_count = estimate_tokens_from_lines(lines_data)
    exceeds_limit = token_count > max_tokens
    
    return exceeds_limit, token_count

def smart_truncate_for_llm(lines_data, max_tokens=12000):
    """
    –£–º–Ω–æ–µ —É—Å–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è LLM —Å —É—á–µ—Ç–æ–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
    1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    2. –ï—Å–ª–∏ –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è - –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
    3. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ —Å tiktoken
    """
    import json
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    json_data = json.dumps(lines_data, ensure_ascii=False, indent=2)
    json_tokens = estimate_tokens(json_data)
    
    if json_tokens <= max_tokens:
        return lines_data, json_tokens, False  # –¥–æ–∫—É–º–µ–Ω—Ç, —Ç–æ–∫–µ–Ω—ã, –±—ã–ª_—É—Å–µ—á–µ–Ω
    
    # –î–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - –ø—Ä–∏–º–µ–Ω—è–µ–º —É—Å–µ—á–µ–Ω–∏–µ
    print(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç: {json_tokens} —Ç–æ–∫–µ–Ω–æ–≤ > {max_tokens}")
    
    total_lines = len(lines_data)
    best_result = None
    
    # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for attempt in range(10):  # –ú–∞–∫—Å–∏–º—É–º 10 –ø–æ–ø—ã—Ç–æ–∫
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É—Å–µ—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –ø–æ–ø—ã—Ç–∫–∏
        reduction_factor = 0.3 + (attempt * 0.1)  # –û—Ç 30% –¥–æ 120% —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        
        if json_tokens > max_tokens * 3:  # –û—á–µ–Ω—å –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
            header_count = max(10, int(total_lines * (0.1 - attempt * 0.01)))
            footer_count = max(15, int(total_lines * (0.15 - attempt * 0.015)))
        elif json_tokens > max_tokens * 2:  # –ë–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
            header_count = max(20, int(total_lines * (0.15 - attempt * 0.02)))
            footer_count = max(25, int(total_lines * (0.2 - attempt * 0.02)))
        else:  # –£–º–µ—Ä–µ–Ω–Ω–æ –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
            header_count = max(30, int(total_lines * (0.2 - attempt * 0.03)))
            footer_count = max(40, int(total_lines * (0.25 - attempt * 0.03)))
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –Ω–µ –±–µ—Ä–µ–º –±–æ–ª—å—à–µ —Å—Ç—Ä–æ–∫, —á–µ–º –µ—Å—Ç—å
        header_count = min(header_count, total_lines // 3)
        footer_count = min(footer_count, total_lines // 3)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–º—ã—Å–ª–∞
        if header_count < 5 or footer_count < 5:
            break
        
        truncated_lines = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        header_lines = lines_data[:header_count]
        for line in header_lines:
            if isinstance(line, dict):
                line_copy = line.copy()
                line_copy['chunk_type'] = 'header'
                truncated_lines.append(line_copy)
            else:
                truncated_lines.append({'text': str(line), 'chunk_type': 'header'})
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        truncated_lines.append({
            'text': f'[... –ü–†–û–ü–£–©–ï–ù–û {total_lines - header_count - footer_count} –°–¢–†–û–ö ...]',
            'chunk_type': 'separator'
        })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–≤–∞–ª
        footer_start = total_lines - footer_count
        footer_lines = lines_data[footer_start:]
        for line in footer_lines:
            if isinstance(line, dict):
                line_copy = line.copy()
                line_copy['chunk_type'] = 'footer'
                truncated_lines.append(line_copy)
            else:
                truncated_lines.append({'text': str(line), 'chunk_type': 'footer'})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä JSON —É—Å–µ—á–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        truncated_json = json.dumps(truncated_lines, ensure_ascii=False, indent=2)
        final_token_count = estimate_tokens(truncated_json)
        
        print(f"‚úÇÔ∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {len(truncated_lines)} —Å—Ç—Ä–æ–∫, {final_token_count} —Ç–æ–∫–µ–Ω–æ–≤")
        
        if final_token_count <= max_tokens:
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —É—Å–µ—á–µ–Ω: {json_tokens} ‚Üí {final_token_count} —Ç–æ–∫–µ–Ω–æ–≤")
            return truncated_lines, final_token_count, True
        
        best_result = (truncated_lines, final_token_count)
    
    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —É–º–µ—Å—Ç–∏—Ç—å –≤ –ª–∏–º–∏—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if best_result:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–º–µ—Å—Ç–∏—Ç—å –≤ –ª–∏–º–∏—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_result[1]} —Ç–æ–∫–µ–Ω–æ–≤")
        return best_result[0], best_result[1], True
    
    # –ö—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
    minimal_lines = lines_data[:10]
    minimal_json = json.dumps(minimal_lines, ensure_ascii=False, indent=2)
    minimal_tokens = estimate_tokens(minimal_json)
    print(f"üö® –ö—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ {len(minimal_lines)} —Å—Ç—Ä–æ–∫, {minimal_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    
    return minimal_lines, minimal_tokens, True

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    test_text = "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤"
    tokens = estimate_tokens(test_text)
    print(f"–¢–µ–∫—Å—Ç: '{test_text}'")
    print(f"–°–∏–º–≤–æ–ª–æ–≤: {len(test_text)}")
    print(f"–¢–æ–∫–µ–Ω–æ–≤ (–æ—Ü–µ–Ω–∫–∞): {tokens}")
